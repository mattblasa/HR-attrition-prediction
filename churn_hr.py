#!/usr/bin/env python
# coding: utf-8

# # IBM HR Analytics Employee Attrition & Performance

# ## Table of Contents

# * [Introduction](#chapter1)
#     * [Background](#Section_1_2)
#     * [Data Dictionary](#section_1_1)
#     * [Problem Statement](#Section_1_2)
# * [Data Processing](#chapter2)
#     * [Libraries](#Section_1_2)
#     * [Experiment Tracking](#Section_1_2)
#     * [Data Preperation](#Section_1_2)
# * [Exploratory Data Analysis](#chapter2)
#     * [EDA Insights](#chapter2)
# * [Data Cleaning](#chapter2)
#     * [Handling Missing Values](#chapter2)
#     * [Outlier Detection](#chapter2)
#     * [Feature Reduction](#chapter2)
# * [Model Building](#chapter2)
# * [Validation](#chapter2)
# * [Conclusions](#chapter2)
# * [Recommendations](#chapter2)

# # Introduction

# ## Background

# ## Data Dictionary
# * AGE 	Numerical Value
# * ATTRITION 	Employee leaving the company (0=no, 1=yes)
# * BUSINESS TRAVEL 	(1=No Travel, 2=Travel Frequently, 3=Tavel Rarely)
# * DAILY RATE 	Numerical Value - Salary Level
# * DEPARTMENT 	(1=HR, 2=R&D, 3=Sales)
# * DISTANCE FROM HOME 	Numerical Value - THE DISTANCE FROM WORK TO HOME
# * EDUCATION 	Numerical Value
# * EDUCATION FIELD 	(1=HR, 2=LIFE SCIENCES, 3=MARKETING, 4=MEDICAL SCIENCES, 5=OTHERS, 6= TEHCNICAL)
# * EMPLOYEE COUNT 	Numerical Value
# * EMPLOYEE NUMBER 	Numerical Value - EMPLOYEE ID
# * ENVIROMENT SATISFACTION 	Numerical Value - SATISFACTION WITH THE ENVIROMENT
# * GENDER 	(1=FEMALE, 2=MALE)
# * HOURLY RATE 	Numerical Value - HOURLY SALARY
# * JOB INVOLVEMENT 	Numerical Value - JOB INVOLVEMENT
# * JOB LEVEL 	Numerical Value - LEVEL OF JOB
# * JOB ROLE 	(1=HC REP, 2=HR, 3=LAB TECHNICIAN, 4=MANAGER, 5= MANAGING DIRECTOR, 6= REASEARCH DIRECTOR, 7= RESEARCH SCIENTIST, 8=SALES EXECUTIEVE, 9= SALES REPRESENTATIVE)
# * JOB SATISFACTION 	Numerical Value - SATISFACTION WITH THE JOB
# * MARITAL STATUS 	(1=DIVORCED, 2=MARRIED, 3=SINGLE)
# * MONTHLY INCOME 	Numerical Value - MONTHLY SALARY
# * MONTHY RATE 	Numerical Value - MONTHY RATE
# * NUMCOMPANIES WORKED 	Numerical Value - NO. OF COMPANIES WORKED AT
# * OVER 18 	(1=YES, 2=NO)
# * OVERTIME 	(1=NO, 2=YES)
# * PERCENT SALARY HIKE 	Numerical Value - PERCENTAGE INCREASE IN SALARY
# * PERFORMANCE RATING 	Numerical Value - ERFORMANCE RATING
# * RELATIONS SATISFACTION 	Numerical Value - RELATIONS SATISFACTION
# * STANDARD HOURS 	Numerical Value - STANDARD HOURS
# * STOCK OPTIONS LEVEL 	Numerical Value - STOCK OPTIONS
# * TOTAL WORKING YEARS 	Numerical Value - TOTAL YEARS WORKED
# * TRAINING TIMES LAST YEAR 	Numerical Value - HOURS SPENT TRAINING
# * WORK LIFE BALANCE 	Numerical Value - TIME SPENT BEWTWEEN WORK AND OUTSIDE
# * YEARS AT COMPANY 	Numerical Value - TOTAL NUMBER OF YEARS AT THE COMPNAY
# * YEARS IN CURRENT ROLE 	Numerical Value -YEARS IN CURRENT ROLE
# * YEARS SINCE LAST PROMOTION 	Numerical Value - LAST PROMOTION
# * YEARS WITH CURRENT MANAGER 	Numerical Value - YEARS SPENT WITH CURRENT MANAGER

# ## Problem Statement
# 
# Attrition is a problem that impacts all businesses, irrespective of geography, industry and size of the company. Employee attrition leads to significant costs for a business, including the cost of business disruption, hiring new staff and training new staff. As such, there is great business interest in understanding the drivers of, and minimizing staff attrition.
# 
# In this context, the use of classification models to predict if an employee is likely to quit could greatly increase the HR’s ability to intervene on time and remedy the situation to prevent attrition. While this model can be routinely run to identify employees who are most likely to quit, the key driver of success would be the human element of reaching out the employee, understanding the current situation of the employee and taking action to remedy controllable factors that can prevent attrition of the employee.
# 
# This data set presents an fictional employee survey from IBM, indicating if there is attrition or not. The data set contains approximately 1500 entries. Given the limited size of the data set, the model should only be expected to provide modest improvement in indentification of attrition vs a random allocation of probability of attrition.
# 
# While some level of attrition in a company is inevitable, minimizing it and being prepared for the cases that cannot be helped will significantly help improve the operations of most businesses. As a future development, with a sufficiently large data set, it would be used to run a segmentation on employees, to develop certain “at risk” categories of employees. This could generate new insights for the business on what drives attrition, insights that cannot be generated by merely informational interviews with employees.

# ## Questions to Answer

# In[ ]:





# # Data Processing

# ## Libraries

# In[5]:


# Import necessary libraries
import pandas as pd
import numpy as np
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib as mpl 
import matplotlib.pyplot as plt
import comet_ml
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor 
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore') # Ignore warning messages for readability


# In[6]:


import pyspark
from pyspark.sql import SparkSession, Window, DataFrame
import pyspark.sql.functions as F
import pyspark.sql.types as T


# ## Experiment Logging - Comet.ml 

# In[ ]:


'''
experiment = Experiment(
    api_key="xCeYnrykwJF1pzF0Rfj8UEzR2",
    project_name="hr_dataset",
    workspace="mattblasa",
)
'''


# ## Data Preparation

# In[7]:


df = pd.read_csv('employee_attrition.csv')
df.head()


# In[8]:


df.describe()


# In[9]:


df.columns


# In[10]:


df.info()


# In[11]:


categ = ['Attrition', 'BusinessTravel', 'Department', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime', 'EducationField']
df_categ = df[categ]


# In[12]:


df_categ


# In[13]:


#print unique cateogries in column 
for col in df_categ:
    print(df[col].unique())


# In[14]:


for col in df_categ:
    print({col : df[col].unique()})


# ### Convert Binary and Categorical Variables

# In[15]:


# Convert binary variables into yes = 1, no = 0 (ref 1)
cols = ['Attrition', 'OverTime', 'Over18']
df[cols] = df[cols].replace(to_replace = ['No', 'Yes'], value = [0, 1])


# In[16]:


#dummy variable based on category 
df_test = pd.get_dummies(data=df, columns=['BusinessTravel', 'Department', 'Gender', 'JobRole', 'MaritalStatus', 'EducationField'])


# In[17]:


df_test = df_test.drop(columns = 'Over18')


# In[18]:


df_test


# # Exploratory Data Analysis

# In[19]:


df_test.info()


# In[20]:


import missingno as ms
ms.matrix(df);


# In[45]:


import seaborn as sns
sns.set_palette(sns.color_palette("Set2", 8))
plt.figure(figsize=(35,20))
sns.heatmap(df.corr(),annot=True)
plt.show()


# In[48]:


df.hist( figsize=(20, 15))


# In[23]:


df_test.columns


# In[49]:


# Plot split bar charts for dummy categorical variables by Churn
df_cat = df_test[['BusinessTravel_Non-Travel', 'BusinessTravel_Travel_Frequently',
       'BusinessTravel_Travel_Rarely', 'Department_Human Resources',
       'Department_Research & Development', 'Department_Sales',
       'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative',
       'JobRole_Human Resources', 'JobRole_Laboratory Technician',
       'JobRole_Manager', 'JobRole_Manufacturing Director',
       'JobRole_Research Director', 'JobRole_Research Scientist',
       'JobRole_Sales Executive', 'JobRole_Sales Representative',
       'MaritalStatus_Divorced', 'MaritalStatus_Married',
       'MaritalStatus_Single','EducationField_Human Resources', 'EducationField_Life Sciences',
       'EducationField_Marketing', 'EducationField_Medical',
       'EducationField_Other', 'EducationField_Technical Degree', 'Attrition']]
count=1
plt.subplots(figsize=(20, 80))
for i in df_cat.columns:
    plt.subplot(20,3,count)
    ax = sns.countplot(x=i, hue='Attrition', data = df_cat)
    legend_labels, _= ax.get_legend_handles_labels()
    ax.legend(legend_labels, ['Attrition No','Attrition Yes'])
    ax.set_xticklabels(('0', '1'))
    count+=1
plt.show();


# In[ ]:





# In[ ]:





# In[ ]:





# # Intial Model

# In[25]:


Xinit = df_test.drop('Attrition', axis = 1)
y = df_test['Attrition'].values


# In[26]:


df_test.info()


# In[27]:


# Importing the libraries
import missingno as msno

# Visualize missing values as a matrix
msno.matrix(df_test);


# In[28]:


df2 = df_test.select_dtypes(include=['uint8'])
df2.columns


# In[29]:


print ("There are", Xinit.shape[1], "independent variables in the initial model.")


# In[30]:


msno.matrix(df_test)


# In[31]:


from sklearn.preprocessing import StandardScaler
Xcinit = sm.add_constant(Xinit)
logistic_regression = sm.Logit(y,Xcinit)
fitted_model1 = logistic_regression.fit()
fitted_model1.summary()


# In[ ]:





# In[32]:


clf = LogisticRegression()
clf.fit(Xinit, y.astype(int))
y_clf = clf.predict(Xinit)
print(classification_report(y, y_clf))


# In[33]:


# Use recursive feature elimination to choose most important features (ref 6)
model = LogisticRegression()
rfe = RFE(model, 10) 
rfe = rfe.fit(Xcinit, y)
print(rfe.support_)
print('\n')
print(rfe.ranking_)
f = rfe.get_support(1) # the most important features
Xfin = Xinit[Xinit.columns[f]] # final features`


# In[34]:


# Look for evidence of Variance Inflation Factors (ref 7) causing multicollinearity
  
# VIF dataframe 
vif_data = pd.DataFrame() 
vif_data["feature"] = Xfin.columns 
  
# calculating VIF for each feature 
vif_data["VIF"] = [variance_inflation_factor(Xfin.values, i) 
                          for i in range(len(Xfin.columns))] 
  
print(vif_data)


# In[35]:


# Re-run the model
Xcfin = sm.add_constant(Xfin)
logistic_regression = sm.Logit(y,Xcfin)
fitted_model2 = logistic_regression.fit()
fitted_model2.summary()


# In[36]:


X_train, X_test, y_train, y_test = train_test_split(Xfin, y.astype(float), test_size=0.33, random_state=101)


# In[37]:


Xcfin = sm.add_constant(X_train)
logistic_regression = sm.Logit(y_train,Xcfin)
fitted_model2 = logistic_regression.fit()
fitted_model2.summary()


# In[38]:


# verification
clf = LogisticRegression()
clf.fit(X_train, y_train.astype(int))
y_clf = clf.predict(X_test)
print(classification_report(y_test, y_clf))


# In[39]:


# View prediction (initial)
clf = LogisticRegression()
clf.fit(Xinit, y.astype(int))
y_clf = clf.predict(Xinit)
print(classification_report(y, y_clf))


# In[40]:


import numpy as np
from sklearn.linear_model import LogisticRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from sklearn.metrics import accuracy_score

import mlflow
import mlflow.sklearn

client = mlflow.tracking.MlflowClient()

try: 
    experiment = client.create_experiment(name = "HR_Logistic Regression")
except:
    print('Experiment Already Exists. Please check folder.')


# In[41]:


with mlflow.start_run(experiment_id='5', run_name='HR_Logistic Regression') as run:
    # Get the run and experiment id 
    run_id = run.info.run_uuid
    experiment_id = run.info.experiment_id
    
    #train, test = train_test_split(data)
    X_train, X_test, y_train, y_test = train_test_split(Xfin, y.astype(float), test_size=0.33, random_state=101)
    
    #Logistic Regression 
    lr = LogisticRegression()
    lr.fit(X_test, y_test)
    
    
    
#Metrics 
    #Precision
    precision = 22

    #Recall 
    recall = 22

    #Accuracy
    acc = 22
    #acc = accuracy_score(y_true, y_pred)    #accuracy
    score = lr.score(X_test, y_test)        #score 
    
    #Confusion Matrix, save confusion matrix to ML runs 
    clf = LogisticRegression()
    clf.fit(X_train, y_train.astype(int))           # logistic Regression Fit
    y_clf = clf.predict(X_test)                    # adad 
    print(classification_report(y_test, y_clf))    # sdsd
        
    #log Metrics 
        #mlflow.log_metric('Name', output) 
    mlflow.log_metric("Precision", precision)
    mlflow.log_metric("Recall", recall)
    mlflow.log_metric("Accuracy", acc)
    mlflow.log_metric("score", score)
   
    
     #Log Model
    mlflow.sklearn.log_model(lr, "model")
    
    
    #Print Metrics 
    print()
    print("Precision: %s" % precision)
    print("Recall: %s" % recall)
    print("Accuracy: %s" % acc)
    print("Score: %s" % score)
    print("Model saved in run %s" % mlflow.active_run().info.run_uuid)
    


# In[42]:


classification_report


# In[ ]:




